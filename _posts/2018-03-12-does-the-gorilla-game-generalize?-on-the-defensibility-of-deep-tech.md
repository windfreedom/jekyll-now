---
layout: post
published: false
---
Geoffrey Moore is a legend in the technology industry. His three most famous books: _Crossing the Chasm_, _Inside the Tornado_, and _The Gorilla Game_ are must-reads for anybody interested in the industry, particularly if you're interested in B2B companies.

Much to my own embarassment, I only managed to get through about half of _Crossing the Chasm_ and none of the rest of his books, even though some of the best VCs of all time - most notably Bill Gurley and Andy Rachleff - highly recommend them. I wanted to rectify this. As Spring Break was fast approaching, I decided to start and finish _The Gorilla Game_.

_The Gorilla Game_ does a great job of giving a high-level overview of the strategic dynamics that have defined competition  and the marketplace battles of the pre-Internet technology industry, encapsulated in companies like Microsoft and Cisco. Whether you want to know about the role of networking effects, lock-in/switching costs, or standards in propelling the most successful companies to dominance, this book is a good starting point. 

But as someone who is growing more interested in so-called 'frontier technology' or 'deep-technology' investing, I read the book with one framing question: "Can the lessons of the Gorilla Game generalize?" That is, are the investing precepts discussed in the book useful for the next generation of pioneering companies? My goal with reading the book - as well as in writing this post - was to identify which trends from the last 40 years of venture investing are likely to persist as we take the leap of faith into a new cycle. 

The reason why this goal matters is two fold: 

1. **The distribution of companies that Sand Hill Road funds is changing.** All the best investment firms, from deep-technology specialists like Lux Capital, to the stalwarts of the VC era, like Sequoia, to the newest entrants, like a16z, have started to make some investments outside the traditional consumer internet and enterprise software categories. I don't have hard data to back this up, but anecdotes support this assertion. For example, Bilal Zuberi, one of the preeminent 'deep-tech' investors discusses this phenomenon [here](https://medium.com/bz-notes/deeptech-frontiertech-is-finding-home-among-vcs-cdbf6baeb96a), and the general proclamation that we're at the [end of the current cycle](http://blog.eladgil.com/2016/07/end-of-cycle.html) of venture investing seems to have become the new norm. Thus, irrespective of what types of new investments the firms have turned to making (deep tech is composed of a heterogenous group of product categories), understanding which principles of successful venture investing translate to this new domain (and which ones don't) is a critical, if underdiscussed topic. 
2. **The competitive dynamics of successive computing and communications revolutions are not guaranteed to persist.** The Gorilla Game delves deep into a particular pattern of competition that seemed to persist amongst high-technology companies. I'll talk about this in more detail later, but in general, the type of competition that creates obscene growth rates, dominant firms in a particular product category, and the accompanying high-market capitalizations that have driven outlier returns for VCs are not guaranteed to occur in every new industry. For example, of the new companies formed in the industrial era, only a few (e.g. AT&T, RCA) brought to market products that exhibited similar competitive dynamics to modern high-technology firms. Venture-backed biotechnology and medical device companies have create a pharmacopeia of life-saving medical interventions over the last 30 years, but the competitive model (and the funding model) they employ is drastically different. And while everyone brings up this example, the clean-tech investing binge clearly demonstrated the dangers of expecting companies with drastically different economic profiles to exit at valuations similar to past venture investments in software and semiconductors. Basically, investing is driven by economic fundamentals and thus if the economic fundamentals of the future technology commercialization change, the funding model for that commercialization should change as well! Thus it's important to try to understand the competitive dynamics that will govern new product categories. 

Now before I begin I do want to note a critical caveat. The categories that currently define 'frontier technology', think artificial intelligence, the Internet of Things, augmented reality, virtual reality, drones, robotics, autonomous vehicles, space/satellites, genomics, etc., are really heterogenous. And becasue of the diversity of this type of investing, it's going to be almost impossible to derive lessons that will apply equally well to all of the product categories listed above. 

With that, let's begin. 

## The Critical Assumptions of The Gorilla Game

In my opinion, the _The Gorilla Game_ attempts to answer 2 fundamental questions that have underpinned the dynamics of technology-investing for the past 40+ years. 

1. **Why do some product categories experience hypergrowth?**
2. **Why does that growth (and the associated economic value) result in a predictable distribution of economic returns amongst firms (the so-called, "Gorilla", "Monkey", and "Chimp")?** 


### Explaining Hypergrowth 

VCs love fast growth. They're under constant pressure to exit their companies within 10 years (at maximum), and because longer exits tank their IRR, they want the company to grow their user base as well as their revenue quickly. Additionally, as this [McKinsey piece](https://www.mckinsey.com/industries/high-tech/our-insights/grow-fast-or-die-slow) (aptly titled, 'Grow Fast or Die Slow') indicates, the market (both in the form of the public markets, as well as private acquirers) pays a premium for fast growth, even more so than for high margins (although the best venture investments have combined both)<sup>1

But a quick look at CAGR rates for various industries will demonstrate that fast growth is most definitely the exception in steady-state economies. I used McKinsey's _Valuation_ text, which placed median company growth rates at anywhere between 2-15% depending on the industry. Double digit growth certainly is good for companies in steady-state, but compared to the CAGRs of 25%+ that VCs like to see from their successful companies, it's pretty paltry. 

So VCs need fast growth, but fast growth is rare. What are they to do?

It just so happens, that with new, discontinuous product categories, there almost always has been a period of hypergrowth associated with the mass-market adoption of that new category. This period of hypergrowth comes with 40% quarter-over-quarter and sometimes up to 200% year-over-year growth rates, which should send VCs salivating. Moore defined this type of growth as follows: "a compressed period of hypergrowth that occurs once in the life of a market, coinciding with the first surge of mass market adoption of any new technology." In the _The Gorilla Game_ model, hypergrowth in these new, discontinuous product categories is based on the dynamics of the technology adoption curve as first delineated by Everett Rogers in _The Diffusion of Innovations_. It's a pretty common model, so I won't belabor the details. Importantly though, it's a very generaizable model - both theoretically and empirically.

Empirically, the 'S-curve' of adoption the model implies (as seen in the picture below) is seen time and time again accross new product categories, irrespective of their relation to the Information and Communication revolutions of the past half-century. While some of the innovations in the picture below are related to that revolution, many are both straight out of the industrial era and competitively/economically very different from the computing product categories (e.g. automobiles, stoves, washing machines).

Inline-style: 
![alt text](https://cdn.vox-cdn.com/thumbor/XaBddHOs5NjyA1bTyiNO-7JIouE=/800x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6139611/blackrock-tech-adoption.png)

Theoretically too, there's no fundamental reason why hypergrowth won't be seen with the new product categories that the Valley is making. The model splits the population of potential users for a discontinous product category into 5 sections (Innovators, Early Adopters, Early Majority, Late Majority, Laggards) , depending on what the individuals' response to the burdens and potential benefits of discontinuous product categories are. The fast growth associated with the entry of a product cateogry into the mass market (the early adopter segment) is due to 2 reasons. 

I'll let the master of innovation diffusion, Everett Rogers, take the first. In _Diffusion of Innovations_ he states that, "The S-shaped curve of diffusion 'takes off' once interpersonal networks become activated in spreading individuals’ subjective evaluations of an innovation from peer to peer in a system. After that point, it is often impossible to stop the further diffusion of a new idea, even if one wished to do so.” Basically, adoption of innovations occurs by exchange of information in peer network, passed from one adopter to the next in a self-propagating chain. This is why Moore puts such an emphasis on _references_ in his book _Crossing the Chasm_. Growth is due to spreading of positive messages through social networks, which inherently create a 'take off effect', though the slope of the takeoff effect will vary between product categories. 

Secondly, the early majority are _deliberate_. They will take time before completely making a decision and almost never lead the decision making process, but _will_ follow with willingness. This desire to follow as a group (as a 'herd' if you will) makes them adopt a new innovation as an entire group, creating a backlog of demand that new product categories will rush to satisfy, thus creating hypergrowth rates. 

Moore's central innovation was to realize that there is a "chasm" between the early adopters and the early majority as a result of the early adopters' willingness to try new ideas, but inability (at least in enterprise markets) to serve as good references for the early majority. 

I'll also note that while Moore has adapted the technology-life cycle model for business-to-business market development (hence the characterization of each of the groups in his books as members of the commercial population, e.g. visionaries as executives with a desire to utilize new technologies to achieve competitive advantage in their own product-markets), the model is _much_ more general than that. 

But the takeaway, irrespective of whether we're thinking about consumer markets or enterprise markets, is the same. There is nothing about the Technology Adoption Lifecycle that is specific to the products of the ICT revolution. Although factors like the innovations' relative advantage, compatibility, observability, etc., will affect the rate of adoption for a new product category (thus influencing the magnitude of the growth rate), VCs can be assured that - assuming the new categories can solve consumer pain points with similar magnitude to that of the ICT revolutions previously - the growth rates in new product categories will on balance be large enough to support their investing model.


### Why One Firm?

However, while Rogers' model of technology adoption can be applied to general product categories (and indeed can be applied to *any* innovation, commercial or otherwise), Moore's definition of hypergrowth is much more specific. To quote Moore again, he states that 'Hypergrowth creates a unique set of marketplace dynamics that frequently will catapult a **single company** into a position of overwhelmingly dominant competitive advantage. This company becomes the gorilla, and the dominant competitive advantages it enjoys will allow it to generate **exceptional returns to its investors for an unusually extended period**."

It's not a secret that venture investors want to invest in companies that are durable and have a moat - monopolies if possible. Investors want to be rewarded for solving a problem for consumers, and without a dominant competitive advantage, any rewards will we be competed away to the cost of capital. And although the distribution of market share and economic rewards can vary in many venture success stories (not every successful venture outcome resulted in a company taking 70+ % market share), Moore isn't wrong that when thinking about the truly dominant companies of the venture-backed era - the ones that have both been true outliers even for the best of funds and forever imprinted their name in the annals of technology history - the distribution of market share in disproportionate manner to the one winner is a common theme. 

But it's worth it to consider that concentration of market share in one major firm and the correspodning allocation of economic returns  is not the norm in most industries. Unlike hypergrowth periods in the adoption of new product categories, the economic characteristics that create dynamics in which a single firm can dominate markets have been particularly localized to the ICT revolution, with precious few examples outside of the communications industry prior to 1950. So what characteristics of high-technology companies have allowed this abnormal distribution of value creation?

In a world dominated by Google, Facebook, Uber, AirBnB, it's no suprise that the obvious answer to this question is "networking effects". But to simply stop there does a disservice to the fundamental reasons why the technology industry - even in an era prior to that of social networking and multi-sided platforms like Facebook and Uber - had examples of near monopolies dominating their respective value chains.

Taking a more nuanced view of competitive dynamics in the ICT industry reveals a couple of key facts that drive the economic structure:

#### Systems Competition

The end products that we've come to associate with the technology industry - the PC, the tablet, the smartphone, routers, and now cloud servers -  are all architecturally linked. That is, they build upon each other. Individual circuits are useless withouth hardware systems to incorporate them, hardware is useless without systems software to organize the hardware, and systems software is in turn useless without application software or content that directly generates value for consumers. The result of this layering is three fold:

1. It's highly unlikely that one firm can make all the components necessary to provide _any given_ end product to users, no matter how trivial the product, which increases the strategic importance of the interlinks between different components of an architecture, each of which might be made by a differing firm. 
2. There are many, many _complementary products_ that form important competitive considerations to any given firms' own offering. 
3. There becomes distinct _vertical_ and _horizontal_ strategies in make/buy decisions. Horizonal companies try to both dominate the competition in a few components of the overall system (e.g. Intel vs. AMD in microprocessors) and improve the value of their components relative to the others making up the system (e.g. microprocessors vs. DRAM - one proved to be more valuable for investors than the other). Vertical companies eschew this logic in order to control their products more closely, often trying to build a substantial amount of the end-product themselves and tightly controlling the components they allow to come from other firms (they also tend to try to own the end user relationship in a much more direct manner to stimulate purchases of their system, whereas only _one_ horizontal firm would have that competency). 

Any time competitive dynamics depend on interfaced components commanding value together but not apart, economists refer to the resulting competitive situation as exhibiting **systems competition**. Systems competition inherently creates **indirect network structure** (e.g. two-sided network structure) and thus the competitive dynamics mirroring that of the multi-sided platforms we know so well today, even if the 'platforms' we think of did not bring two human parties together to exchange services, but instead brought together different components of a system. This is because in these systems, certain core components, which we have tended to call 'platforms,  **interface** with many other complements and are core to their functioning well together. 

Consider two components, A (being produced by Firms 1 and 2) and B (representing a variety of complementary components), and two consumers C and D. Consumer C might not directly care about the relative market shares of the Firms 1 and 2 producing component A, but they know that the more popular product A1 is, the more likely it will interface with component B, which adds value to the overall product offering that Consumer C purchases. Thus, assuming product A1 commands more market share, Consumer C will on balance (certus paribus) pick product A1. Correspondingly, the resultling increase in market share of product A1 makes it more valuable for firms producing any complementary components from set B  to interface with component A1 over A2, then increasing the value of A1 to the next consumer D. The cycle that this  kicks off is emblematic of the competitive dynamics of product categories governed by indirect or 'cross-side' network effects. It's also why, so long as one can be sure that one's own particular component is the key asset (the 'platform' as it were) to which other components interface (i.e., it 'sets' the architecture) then often being modular and opening up your system so that it can connect to many complements is a good idea, even if it sacrifices lock-in advantages. 

The canonical example of these dynamics occured in PC operating systems. But even in the microprocessor product category, this type of indirect network effect persisted, catapaulting Intel to near monopoly status. Even in this decidedly _non-networked_ business (in the traditional sense of multi-sided platforms or true communication networks), because Intel's microprocessors  interacted with every potential PC peripheral, every core component (e.g. memory), and most importantly was only compatible with Windows - the dominant operating system -  their microprocessor product category had many complementary products it could leverage into an ecosystem that consumers were loathe to abandon. The value of Intel's microprocessors to its customers in the PC-value chain (mainly OEMs) thus increased with the size and value of the ecosystem of complementary products (both in the hardware layer as well as above, in the software layer) that were built to run with Intel's products alone. And as more consumers used Intel's microprocessors, its behooved those connecting hardware and software makers to connect with Intel's processors above others. Thus a 'standard' was born. 

Moore argues that this 'de-facto' standardization derives from early-majority consumer conservative impulses, which lead them to want to standardize their products and buy only from market leaders. I definitely think this is also an important consideration. But even outside of demand side considerations, supply side economic characteristics inherent in systems competition can create de facto standardization. 

In my opinion, the important takeaway from this is simply that when the end products bought by end users (be it enterprise users or consumer users) are made up of complex systems of components that have little value outside of that system (complements), there are opportunities for certain components to exhibit indirect networking effects that can result in the type of disproportionate market share and market capitalization allocations that have been seen in the past. 

The good news for deep-technology investors is that most of the new 

### A Sidebar: Are Deep Technology Product Categories Discontinuous?


1. Why this prioritization of growth over margin structure? The reasons are many and hard to completely delineate. But here's a summary. Firstly, growth means that you can have larger future revenues and thus cash flows, which makes any DCF model happy. Secondly, growth is a powerful signal that the company has tapped into a huge and desperate pain point. Investors are usually willing to assume that margin structures will work themselves out because lightning fast growth is a signal that the company has a defined competitive advantage over competitors and thus will likely not have margin issues. That is, past experience has shown a strong correlation between fast growth and high margins - companies like Microsoft, Cisco, etc., have all grown fast, but combined this growth with high margins. This is the fundamental crux of _The Gorilla Game_. Lastly, growth is **hard**.  If you crack open McKinsey's _Valuation_ textbook and flip to the section on growth, you'll come away with one key takeaway - market share shifts in a steady state competitive environment are marginal at best, even for firms with a distinct advantage manifesting in price or lower costs. So investors are willing to pay for reveue growth in excess of general economic growth rates because its so rare. 

